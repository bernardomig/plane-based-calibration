\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage[utf8]{inputenc}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{cleveref}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage[draft]{todonotes}
\usepackage{bm}

% \def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Lidar to end-effector calibration approach\\ using planar based features}

\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
}

\maketitle

\begin{abstract}
\dots
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}\label{section:introduction}

The Laser Range Finder, or LRF, is one of the most important sensor in the field of robotics, employed in application such as SLAM and 3D reconstruction. Its use is increasing due to their continuously decrease in price, new research and development and its fundamental role in emerging technologies such as autonomous driving.

% TODO: add references
% TODO: 
In this paper, a 2D LRF is used in a 3D reconstruction system, to produce high-detail and accurate models of real scenes. The common solution involves the integration of the LRF on a robotic arm, such as a PTU, to enable a full 3D reconstruction. However, one of the challenges of this approach is the calibration of the position of the LRF, relative to the coordinate frame of the robotic arm. This process is known as the LRF extrinsic calibration.

% TODO: add measuring instruments, explain further, add examples
Typically, the standard procedure to do this calibration relies on a study of the geometry of such system, using a measuring instrument. However, this approach has several disadvantages: it is time-consuming, the instruments required are expensive and generally not easily available. Therefore, this solution is often not feasible for research work and prototype development.

Therefore, reliable and automatic methods for the LRF extrinsic calibration are required. Most of these methods rely on the comparison between the structure of the reconstructed points with a known geometry, e.g. a marker. In \cite{Kim2013}, a marker formed by two perpendicular planes is used, and their points are segmented from the reconstructed point cloud. Then, the relation between normals of these planes are used as the objective function for the calibration. However, this method presents some limitations: first, ensuring that two planes are perpendicular with a small error can be very hard (carboard ); second, it is not straight-forward how to increase the number of planes; finally, by using a small marker, only a small portion of the space is occupied, which means that only a portion of the sensors' full field of view is used, which may lead to a sub-optimal result.

{\color{red}
Completar com mais estado da arte}

In this paper, a new method for the extrinsic calibration of a LRF w.r.t. a robotic arm is presented, which makes used of the reconstructed scene geometry, more specifically, its planar regions. Then, the evaluation of this regions serves as the objective function for an optimization procedure, which searches the position of the LRF which minimizes the deformations of the reconstructed geometry. Results will show that the proposed approach is capable of generating accurate reconstructed scenes.

% TODO
The remainder of this paper will be structured as follows...

\section{Problem Formulation}\label{section:problem-formulation}

The LRF is a sensor that outputs several laser scans. Each laser scan is a set of range measurements taken along several directions but at the same time, all coincident with a plane, the scan plane. That is in fact why these equipments have  a 2D nature. Thus, for each scan, a 2D slice of the scene is measured. Since the LRF is mounted on top of an actuated kinematic chain (e.g. a PTU or robotic manipulator), the movement of this chain positions the LRF in different poses in the 3D space. Through the accumulation of the several laser scans, a dense point cloud of the scene is produced. 

The process of accumulation is, in essence, based on the concatenation of 3D points from multiple scans. However, the grouping of these points requires that the points of each laser scan are transformed to a common, static, reference frame, the map frame. In order to achieve this, one must apply a geometric transformation to each 3D point $\mathbf{p}^{(i)}$ measured in scan $i$, represented in the LRF's local coordinate system $l$. The transformed point $\mathbf{q}^{(i)}$, defined in the map reference frame, is determined by:
%
\begin{equation}
    \mathbf{q}^{(i)} = \, ^{m}\mathbf{T}_{e}^{(i)} \cdot ^{e}\mathbf{T}_{l} \cdot \mathbf{p}^{(i)},
\end{equation}
%
\noindent where ${^{m}\mathbf{T}_{e}}^{(i)}$ is the transformation between the map and the end effector coordinate frames, which is dynamic and given by the process that receives a description of the kinematic chain, the joint values at the scan $i$, and computes the direct kinematics. In turn, $^{e}\mathbf{T}_{l}$ denotes the position of the LRF w.r.t the end effector. 

Note that, although this transformation is static, it influences the coordinates of the points $\mathbf{q}^{(i)}$, and therefore is crucial to the accuracy with which three dimensional objects are represented in the accumulated point cloud.
Thus, an accurate estimation of transformation $^{e}\mathbf{T}_{l}$ is paramount to process of 3D reconstruction. We refer to this procedure of estimation the LRF to end effector transformation, as the Lidar to end-effector extrinsic calibration. 
To best of our knowledge, there is no straight forward, off-the-shelf methods for 
conducting this calibration, as will be detailed in \cref{sec:related-work}.

%TODO: Colocar aqui uma figura ilustrativa do problema

In this work, we propose a novel approach to perform Lidar to end-effector extrinsic calibration. The approach is based on an optimization procedure which computes the transformation that generates planar point cloud sections in areas which are marked as planes. To this end, several existing scene structures may be used, such as walls, ceilings or pavements. This is an additional advantage of our proposed method: it does not require dedicated calibration targets.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Approach}\label{sec:proposed_approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The working principle of this method relies on the following assunption: in a good calibration, the deviation of a measured point set w.r.t. to the real scene geometry should be minimal. This paper focuses on planar geometries because of two reasons:  they are easily segmented from the full point cloud; and it is straight-forward to compute a similarity metric between the point cloud and the expected planar geometry. In other words, in a point set representing a planar surface, the deviation from the points to the planar surface should be the lowest, if the calibration is correct.

This method can then be formulated as an optimization problem: for each extrinsic calibration transformation $\mathbf{T}$, corresponds a point cloud $\mathcal{P}$. This point cloud is evaluated by a cost function, which determines quantitatively how good each generated point cloud is. Finally, an optimizer will find the transformation $T$ that minimizes the loss function. Therefore, this method can be defined as:
%
\begin{equation}
    \mathbf{T} = \underset{T}{\mathrm{argmin}} \left\{ \mathrm{\sigma}(\textbf{P}) \right\},
\end{equation}
%
\noindent where % definir as variaveis



Falar dos v√°rios componentes:

\begin{itemize}
    \item Parametros
    \item Cost Fn
    \item Optimizer
\end{itemize}

\subsection{Paramerization of the Transformation}

The parameters in this calibration are six values that define a geometric transformation in space, which is, in the end, cast back to the transformation matrix $\bm{T}$. This transformation is decomposed into two components, a translation and a rotation. The translation can be represented as the translation vector $\mathbf{t} = \left[t_x, t_y, t_z\right]$, and the rotation is be represented as a $3 \times 3$ rotation matrix $\bm{R}$. Since a rotation matrix has only $3 \times 3 = 9$ elements but only 3 degrees of freedom, another parameterization has to be used to represent a rotation. Popular parameterization for rotations are euler angles, quaternions and axis/angle. However, not all representations are suitable for an optimization. In fact, in~\cite{hornegger99} the term fair parameterization was introduced: a parameterization is called fair, if it does not introduce more numerical sensitivity than the one inherent to the problem itself. Therefore, fair parameterizations are required for optimizations, as they increases the chances for convergence. For example, euler angles, which are probably the most used angle parameterizations, are not suitable~\cite{schmidt01}, because they do not yield smooth movements. Each rotation is non-unique and, most notably, there are singularities, so-called \textit{gimbal-lock} singularities, where one degree of freedom is lost~\cite{schmidt01}. Also, quaternions are not suitable for optimizations, because quaternions have $4$ components which are constrained to an unitary length. Despite being a fair parameterization, quaternions introduce some complexity in the algorithm to handle this constraint, so they are not usually used for optimizations~\cite{schmidt01}.

The axis/angle parameterization is the most widely used to represent a rotation in optimization procedures, as it is a fair parameterization and has only three components. Any rotation can be represented as a rotation around an axis $a$, by an angle $\theta$. Since $\bm{a}$ only represents the direction of the rotation (hence only has 2 degrees of freedom), it can be combined with the angle $\theta$ into a single rotation vector $\bm{\omega} = \left[\omega_1, \omega_2, \omega_3\right]$, as follows:

\begin{equation}
    \label{eqn:axis-angle}
    \begin{aligned}
        \theta & = |\bm{\omega}| \\
        \bm{a} & = \frac{\bm{\omega}}{|\bm{\omega}|}
    \end{aligned}
\end{equation}

Computing the rotation matrix from $\omega$ is done using the Rodrigues' formula~\cite{schmidt01}:
%
\begin{equation}
    \textbf{R} = \bm{I} + \frac{\sin \theta}{\theta} \bm{\Omega} + \frac{1 - \cos \theta}{\theta^2} \bm{\Omega},
\end{equation}
%
\noindent
where I is the $3\times3$ identity matrix, and $\bm{\Omega}$ is given by:
%
\begin{equation}
    \bm{\Omega} = \left[
        \begin{array}{ccc}
            0  & -\omega_3 & \omega_2 \\
            \omega_3 & 0   & -\omega_1 \\
            -\omega_2 & \omega_1 & 0 \\
        \end{array}
    \right].
\end{equation}


In conclusion, the parameter vector to be optimized will have six values: three representing the translation $\bm{t}$ and the rotation vector $\bm{\omega}$ represented in the axis/angle representation. So, the parameter vector $\bm{x}$ is defined as:

\begin{equation}
    \bm{x} = \left[t_1, t_2, t_3, \omega_1, \omega_2, \omega_3\right].
\end{equation}

%TODO: Colocar dentro 
\subsection{Segmentation}

This calibration method uses an acquisition as its dataset, which is a significant advantage, since no calibrations patterns and no special apparatus is required, like chessboards or other markers. Also, a point cloud has to be generated using a estimation of the calibration transformation. This point cloud does not have to be geometrically accurate but the geometry should be perceivable for the plane segmentation, which is done manually prior to the calibration. In this work, the software CloudCompare was used to segment the point cloud into multiple planes, and the data was saved as a scalar index in each point. An example of a segmentation can be seen in \cref{figure:cluster-segmentation-1}, where each cluster is represented with a different color.

The segmentation was done manually because most segmentation algorithms, for example the RANSAC algorithm, were not capable of achieving a reliable segmentation for the initial estimate, because the point cloud had significant deformation. In addition, manual segmentation is easy to do and accurate, considering that it is a one-time process.

During the optimization, this segmentation serves as a blueprint for all the segmentations. Each point cloud is generated in the same way, so the sequence of points is always the same. Therefore, it is always possible to match any point on the generated point cloud to the point in the segmented point cloud, and get the corresponding cluster index for all the points.

\begin{figure}[h]
    \centering
%    \includegraphics[width=.8\textwidth]{cluster-segmentation-1}
    \caption{Example of a plane segmentation, where each color represents a cluster.}
    \label{figure:cluster-segmentation-1}
\end{figure}

\subsection{Cost Function}
\label{section:calibration-cost-function}

The cost function is a measure used in optimization that compares the result of a model with its expected result, and returns a value that describes the dissimilarity between the two. More concretely, in this calibration the cost function has two steps: the cost is computed for each cluster and then the cost of all the clusters is combined into a single value, which is the final cost of the point cloud.

In an inicial step, the plane equation for each cluster is computed, using the Principal Component Analysis method, or PCA. First the centroid $\bar{p}$ of each plane is found, which is the same as the mean value of all the points $p: (x, y, z) \in \textbf{R}^3$:

\begin{equation}
    \bar{p} = \sum_{i}{p_i}.
        \label{eqn:centroid-plane}
\end{equation}

Then, the covariance matrix $\mathcal{C}$ is calculated:

\begin{equation}
    \mathcal{C} = \sum_{i}{(p_i - \bar{p}) \otimes (p_i - \bar{p})} \footnote{$\otimes$ is the outer tensor product.}.
        \label{eqn:covariance-matrix}
\end{equation}

Then, the principal axes of the plane is find by an eigen decomposition of the covariance matrix. The smallest eigenvalue $\lambda_3$ will be the variance $\sigma^2$ of the cluster. In other words, $\sigma^2$ is the mean square of the orthogonal distance of all points in the cluster to the plane. So, $\sigma^2$ can be a quantitative factor to measure the cost or each cluster. Formally, let us admit that the $\sigma^2$ has two components: the statistical error of the laser sensor $\sigma^2_{sensor}$, which is not affected by the calibration and a second component $\sigma^2_{calib}$, which depends of the calibration error. Thus, the idea is that, by minimizing $\sigma^2$, a exact calibration can be obtained. For this calibration, however, the value $\sigma$ was used instead of $\sigma^2$, which is known as the Root Mean Square Deviation, or RMS. Therefore, the loss of each cluster will be the $\sigma$ value.

Next, the scores of the clusters are combined into a scalar value, which is the error of the point cloud. The method found was to, again, calculate the RMS of the values of the partial losses $\textrm{loss}_i$, according to \cref{eqn:rms}. This value is expected to be minimal when all the partial losses are minimal which, according to this hypothesis, corresponds to a correct calibration.

\begin{equation}
    \label{eqn:rms}
    RMS = \sqrt{\sum_{i}^{N}{\textrm{loss}_i^2}}
\end{equation}

\subsection{First Guess}

This optimization was quite robust to the initial parameters, so the first guess was always a null translation and the rotation was done doing a visual inspection of the laser scanner, using angles multiples of \SI{90}{\degree}.

\subsection{Optimizer}

The optimization is performed using the Powell's method, described in \cite{powell64}. This method finds a local minimum of a multi-dimensional unconstrained function, and does not require the gradient of this function (is unknown in this problem), which fits this particular optimization. This method is implemented in the python scientific library SciPy\footnote{See the Scipy reference in \url{https://docs.scipy.org/doc/scipy/reference/optimize.minimize-powell.html}.}.


% maybe describe the actual equipament used

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}\label{sec:results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
    \item Figura com an√°lise qualitativa (before / after)
    \item Quantitativa (dispers√£o e ortogonalidade)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and Future Work}\label{sec:conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
    \item Optimizador lento
\end{itemize}




\bibliographystyle{IEEEtran}
\bibliography{references/refs}


\end{document}
